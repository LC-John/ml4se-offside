# Paper discussion: Structured Neural Summarization

#### Paper

* Appeared in International Conference on Learning Representations (ICLR)
* Cited 7 time according to Google Scholar
* The work is important because it improves the state-of-the-art seq2seq encoders for code summarization. It is also 
important to note that most of the work prior was mostly focusing on decoders.

#### People
* Patrick Fernandes - MPhil from Cambridge, AI Resident at Microsoft Research Cambridge 
* Miltos Allamanis - Senior researcher at Microsoft Research in Cambridge, UK and part of the Deep Program Understanding project.
* Marc Brockschmidt - Researcher at Microsoft Research in the All Data AI and Programming Principles and Tools groups in Cambridge

#### Motivation
Generating natural language from source code provides multiple benefits. Shorter descriptions can be used to evaluate
or generate method names, longer ones can be used to create documentation for the code and therefore increase its
usability.  

#### Research method
This work contributes in a novel way in order to enhance the current ML models in the aspect of 
long-distance relationships and improve the summarization of code. 
The authors gather C# data by mining 23 open-source repositories from GitHub. They also use a small Java dataset
generated by other authors.

#### ML4SE techniques used
The authors augment an existing seq2seq LSTM encoder by combining it with GGNN. In addition, authors cleverly use the
relationships between code by constructing a graph out of it before feeding into the model. 

#### Results
The results indicate that the Graphical Neural Network is simple, yet effective for Java, C# and CNN/DailyMail dataset
with respect to the external baselines. In addition, the structured approach used by authors provides consistency that 
other methods tend to lack.

#### Implications
The results indicate that using a structured approach and combining LSTM encoder with GGGN provide promising results. 
This opens opportunities for future work and more accurate results on creating method names and documentation for 
source code. 

#### Techincal questions
* How are the nodes of the graph embedded?
* Why can this encoder be used with any decoder?
* What does the coverage mechanism do?

#### Discussion points
* Why didn't the authors also try a decoder with coverage? Is it too hard to implement? Would the results improve a lot?
* How would this approach perform compared to code2vec or code2seq?
* Why is there such a big difference between Java and C# (Statistical values are much higher for the C# dataset)?
* Are structural annotations worth it or are they over-engineering?
* Could a ConvNet-Sequence hybrid model work as an alternative to Graph-Sequence?
* Would the removal of coreferencing make the results much worse?


#### Summary of the discussion
* Mostly technical details of the paper with Miltiadis. We discussed how GNNs work, the effect of using coverage mechanisms and finally how the structure of the graph is used to generate an input for the RNN.
* We also discussed that the reason for the difference in results for Java and C# was that the C# dataset was originally generated by another research group who put considerable effort into generating more informative graphs whereas the graph for the Java dataset were generating by the authors who did not focus as much on generating as detailed graphs.
* Using coverage will considerably slow down the training of the model since it complicates the output generation. When used it is often only used in the later stages of training, for fine-tuning of the model. This would've complicated the training process and was therefore skipped.
* Embeddings would probably improve the results, however that raises the question of "which embedding to use?" There are no widely accepted embeddings for code and different embeddings will have slightly different effects on the final outcome. Researching different embeddings was mentioned as a possible future (or current) research area.




# Paper discussion: Summarizing Source Code using a Neural Attention Model

#### Paper

* Appeared in Annual Meeting of the Association for Computational Linguistics (ACL)
* Cited 93 times according to Google Scholar
* Why is it important?

#### People
_A brief profile of the main authors_

#### Motivation
_Why do this research?_

#### Research method
_What does the paper do?_

#### ML4SE techniques used
_How does the paper use ML techniques_

#### Results
_What does the paper find?_

#### Implications
_Why are the results important?_

#### Techincal questions
* Why do the authors remove variable names in SQL examples and keep them in C# examples?
* How does the attention mechanism work? What is it paying attention to?
* What is the advantage of using beam search in the GEN task? And how does beam search work?

#### Discussion points
* Is the model too small for the complexity of the task?
* Is this approach scalable
* Tokens with frequency < 3 are set as unknown, why 3 and what effect will changing this threshold have?
* Why donâ€™t the authors compare their solution to other solutions that were designed for code?
* Is their data gathering approach good? Are there other models for which we can use a similar data gathering approach?
* One of the artifacts of the data gathering approach is that all the results are in question format. Could there be any other artifacts due to this approach?

#### Summary of the discussion
* A short summary of Beam search. A specific note from Miltiadis that using log-probabilities is preferred to using normal probabilities since with normal probabilities you have a higher risk of at some point reaching a probability of 0 and then you are stuck.
* A discussion about the size/complexity of the model. Generally having a smaller/simpler model is better, especially if the model is able to acheive comparable results to a more complicated one. However, the size of the dataset must also be taken into account, applying a complicated model to a small dataset will lead the model to simply memorizing the features of that dataset.
* We also discussed the concept of "size" of a Machine Learning model, the concept of parameters and how many parameters there are in a model. A parameter is essentially anything that changes as the model learns.
* The reason for discarding rare tokens was discussed, this was probably done to avoid overfitting on the rare tokens. The exact threshold value of 3 was not really addressed.
* Their comparison to Allamanis's model was discussed, and that model was focused on the Code to Language problem which explains why the results for that problem were within the margin of error whereas the scores for the Language to Code problem were vastly different.
* We discussed the problem of data collection. In this paper the approach used is quite restrictive, only using the title of the question and accepted answers with a single code tag. There were no definitive answers, this is a difficult problem. One mention was made that the type of problems on StackOverflow tend to be simple to medium complexity and the more complex problems are usually answered with multiple code tags and so the dataset probably only contains simple problems.
* One downside of using StackOverflow is that duplicate questions are not allowed/closed and so there is usually not more than one example of each question which may have some impact on learning.
* This paper was at its time a leading paper in the field but today it would be considered a bachelor thesis. The field is moving very quickly and we need to move quickly as well.
