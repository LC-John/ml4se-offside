\section{Relevant Literature} \label{sec:relevant_literature}
\textit{Code2Vec} by Alon et al. \cite{alon2019code2vec} is a state-of-the-art deep neural network model used to create fixed-length vector representations (\textit{embeddings}) from code. The embeddings of similar code snippets encode the semantic meaning of the code, meaning that similar code has similar embeddings. 

The model relies on the paths of the Abstract Syntax Tree (AST) of the code. While the idea of using AST paths to generate code embeddings is also used by other authors such as Hu et al. \cite{hu2018deep}, Code2Vec is superior due to the novel way of using AST paths. Instead of linearizing paths as \cite{hu2018deep}, Code2Vec authors use a path-attention network to identify the most important paths and learn the representation of each path while simultaneously learning how to aggregate a set of them.

The authors make use of embedding similarity of similar code to predict method names. The model is trained on a dataset of 12 million Java methods and compared to other competitive approaches. It significantly outperforms them by having approximately 100 times faster prediction rate at the same time having a better F1 score of 59.5 at method naming. While they only tested their model for method naming, the authors believe that there are a plethora of programming language processing tasks the model can be used for.

\textit{DeepBugs} by Pradel et al. \cite{pradel2018deepbugs} uses a deep learning model to identify bugs related to swapped function arguments, wrong binary operators and wrong operands in binary operation. The model creates an embedding from the code, but instead of using ASTs like Code2Vec, the embedding is created from specific parts of the code. For example, embeddings for identifying swapped function arguments are created from the name of the function, names and types of the first and second arguments to the function with their parameter names, and name of the base object that calls the function.

They use a Javascript dataset containing likely correct code. In order to generate negative examples for training, they use simple transformations to create likely incorrect code. The authors formulate bug detection as a binary classification problem and evaluate the embeddings quantitatively and qualitatively. The approach yields an effective accuracy between 89.06\% and 94.70\% depending on the problem at hand.